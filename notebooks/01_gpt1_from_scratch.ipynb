{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT-1 From Scratch with TensorFlow / Keras\n",
        "\n",
        "This notebook implements a minimal **GPT-1**-style decoder-only transformer for\n",
        "causal language modelling (next-token prediction). We build every component from\n",
        "first principles using Keras layers, train on a small text corpus, and generate\n",
        "text at the end.\n",
        "\n",
        "### GPT-1 recap (Radford et al., 2018)\n",
        "\n",
        "| Component | Detail |\n",
        "|---|---|\n",
        "| Architecture | Decoder-only Transformer |\n",
        "| Attention | Causal (masked) multi-head self-attention |\n",
        "| Positional info | **Learned** positional embeddings |\n",
        "| Pre-training task | Autoregressive language modelling (next-token prediction) |\n",
        "| Original size | 12 layers, 12 heads, 768 hidden dim (117 M params) |\n",
        "\n",
        "We'll use a **much smaller** variant so it trains quickly on a single GPU\n",
        "(or even a CPU)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 0 — Environment & Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Quieten TF logging (3 = suppress errors, 2 = suppress warnings)\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "\n",
        "# ---------- GPU / CUDA configuration ----------\n",
        "# Uncomment the next line to force CPU-only if GPU causes issues:\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError:\n",
        "        pass  # already initialized — safe to ignore\n",
        "\n",
        "print(f\"Python  : {sys.version}\")\n",
        "print(f\"TF      : {tf.__version__}\")\n",
        "print(f\"Keras   : {getattr(keras, '__version__', 'bundled with TF')}\")\n",
        "print(f\"Device  : {'GPU — ' + gpus[0].name if gpus else 'CPU'}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python  : 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0]\n",
            "TF      : 2.16.1\n",
            "Keras   : bundled with TF\n",
            "Device  : CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1 — Hyperparameters\n",
        "\n",
        "We define a small model that is practical for experimentation.\n",
        "Feel free to scale these up if you have a GPU with more memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---- Model ----\n",
        "NUM_LAYERS   = 4          # transformer blocks\n",
        "NUM_HEADS    = 4          # attention heads\n",
        "HIDDEN_DIM   = 128        # embedding / hidden dimension (d_model)\n",
        "FF_DIM       = 512        # feed-forward inner dimension\n",
        "DROPOUT_RATE = 0.1\n",
        "MAX_SEQ_LEN  = 128        # context window\n",
        "\n",
        "# ---- Training ----\n",
        "BATCH_SIZE     = 64\n",
        "EPOCHS         = 20\n",
        "LEARNING_RATE  = 3e-4\n",
        "WARMUP_STEPS   = 200\n",
        "\n",
        "# ---- Reproducibility ----\n",
        "SEED = 42\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2 — Dataset & Tokenisation\n",
        "\n",
        "We use a small, self-contained dataset so the notebook runs end-to-end without\n",
        "large downloads. We use the **TinyShakespeare** corpus (~1 MB of text) which is\n",
        "a classic benchmark for small language models.\n",
        "\n",
        "For tokenisation we build a simple **character-level** tokeniser. This keeps\n",
        "things transparent — every token is a single character, so you can easily\n",
        "inspect inputs and outputs. (A BPE / WordPiece tokeniser would be more\n",
        "efficient on a real task.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Download TinyShakespeare\n",
        "DATA_URL = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "data_path = keras.utils.get_file(\"tinyshakespeare.txt\", DATA_URL)\n",
        "\n",
        "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(f\"Corpus length: {len(text):,} characters\")\n",
        "print(f\"First 200 chars:\\n{text[:200]}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus length: 1,115,394 characters\n",
            "First 200 chars:\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build character-level vocabulary\n",
        "chars = sorted(set(text))\n",
        "VOCAB_SIZE = len(chars)\n",
        "\n",
        "char_to_id = {ch: i for i, ch in enumerate(chars)}\n",
        "id_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "def encode(s: str) -> list[int]:\n",
        "    \"\"\"Encode a string into a list of integer token IDs.\"\"\"\n",
        "    return [char_to_id[c] for c in s]\n",
        "\n",
        "def decode(ids: list[int]) -> str:\n",
        "    \"\"\"Decode a list of integer token IDs back into a string.\"\"\"\n",
        "    return \"\".join(id_to_char[i] for i in ids)\n",
        "\n",
        "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
        "print(f\"Characters: {''.join(chars)}\")\n",
        "print(f\"\\nEncode 'hello': {encode('hello')}\")\n",
        "print(f\"Decode back:     {decode(encode('hello'))}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 65\n",
            "Characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "\n",
            "Encode 'hello': [46, 43, 50, 50, 53]\n",
            "Decode back:     hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Encode the entire corpus and create train / val splits\n",
        "data = np.array(encode(text), dtype=np.int32)\n",
        "\n",
        "split_idx = int(0.9 * len(data))\n",
        "train_data = data[:split_idx]\n",
        "val_data   = data[split_idx:]\n",
        "\n",
        "print(f\"Train tokens: {len(train_data):,}\")\n",
        "print(f\"Val   tokens: {len(val_data):,}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train tokens: 1,003,854\n",
            "Val   tokens: 111,540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def make_dataset(token_ids: np.ndarray, seq_len: int, batch_size: int) -> tf.data.Dataset:\n",
        "    \"\"\"Create a tf.data.Dataset of (input, target) pairs for language modelling.\n",
        "\n",
        "    For each window of `seq_len + 1` tokens:\n",
        "      - input  = tokens[:-1]\n",
        "      - target = tokens[1:]   (shifted by one position)\n",
        "    \"\"\"\n",
        "    ds = tf.data.Dataset.from_tensor_slices(token_ids)\n",
        "    # Create overlapping windows of seq_len + 1\n",
        "    ds = ds.window(seq_len + 1, shift=seq_len, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(seq_len + 1))\n",
        "    ds = ds.map(lambda w: (w[:-1], w[1:]), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.shuffle(10_000).batch(batch_size, drop_remainder=True)\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "train_ds = make_dataset(train_data, MAX_SEQ_LEN, BATCH_SIZE)\n",
        "val_ds   = make_dataset(val_data,   MAX_SEQ_LEN, BATCH_SIZE)\n",
        "\n",
        "# Inspect one batch\n",
        "for x, y in train_ds.take(1):\n",
        "    print(f\"Input  batch shape: {x.shape}\")\n",
        "    print(f\"Target batch shape: {y.shape}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input  batch shape: (64, 128)\n",
            "Target batch shape: (64, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3 — Model Architecture\n",
        "\n",
        "We build GPT-1 bottom-up from its sub-components:\n",
        "\n",
        "1. **Causal self-attention** — each position can only attend to itself and\n",
        "   earlier positions (autoregressive masking).\n",
        "2. **Transformer block** — attention → residual + LayerNorm → FFN → residual +\n",
        "   LayerNorm.\n",
        "3. **GPT model** — token embedding + learned positional embedding → N\n",
        "   transformer blocks → linear projection to vocabulary logits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 — Causal Self-Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class CausalSelfAttention(layers.Layer):\n",
        "    \"\"\"Multi-head causal (masked) self-attention.\n",
        "\n",
        "    Each position can only attend to positions <= itself, which is enforced\n",
        "    by adding a large negative bias to future positions before the softmax.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim: int, num_heads: int, dropout: float = 0.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        assert hidden_dim % num_heads == 0, \"hidden_dim must be divisible by num_heads\"\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_dim // num_heads\n",
        "\n",
        "        self.qkv_proj = layers.Dense(3 * hidden_dim, use_bias=False, name=\"qkv_proj\")\n",
        "        self.out_proj = layers.Dense(hidden_dim, use_bias=False, name=\"out_proj\")\n",
        "        self.attn_dropout = layers.Dropout(dropout)\n",
        "        self.resid_dropout = layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        B = tf.shape(x)[0]      # batch size\n",
        "        T = tf.shape(x)[1]      # sequence length\n",
        "        C = x.shape[-1]         # hidden_dim (static)\n",
        "\n",
        "        # Project to Q, K, V in one shot\n",
        "        qkv = self.qkv_proj(x)                              # (B, T, 3*C)\n",
        "        qkv = tf.reshape(qkv, (B, T, 3, self.num_heads, self.head_dim))\n",
        "        qkv = tf.transpose(qkv, perm=(2, 0, 3, 1, 4))      # (3, B, H, T, D)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]                   # each (B, H, T, D)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        scale = tf.math.sqrt(tf.cast(self.head_dim, dtype=q.dtype))\n",
        "        attn = tf.matmul(q, k, transpose_b=True) / scale    # (B, H, T, T)\n",
        "\n",
        "        # Causal mask: prevent attending to future tokens\n",
        "        causal_mask = tf.linalg.band_part(\n",
        "            tf.ones((T, T), dtype=attn.dtype), -1, 0\n",
        "        )  # lower-triangular\n",
        "        attn = attn * causal_mask + (1.0 - causal_mask) * (-1e9)\n",
        "\n",
        "        attn = tf.nn.softmax(attn, axis=-1)\n",
        "        attn = self.attn_dropout(attn, training=training)\n",
        "\n",
        "        # Weighted sum of values\n",
        "        out = tf.matmul(attn, v)                             # (B, H, T, D)\n",
        "        out = tf.transpose(out, perm=(0, 2, 1, 3))           # (B, T, H, D)\n",
        "        out = tf.reshape(out, (B, T, C))                     # (B, T, C)\n",
        "\n",
        "        out = self.out_proj(out)\n",
        "        out = self.resid_dropout(out, training=training)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 — Transformer Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    \"\"\"A single GPT-1 transformer block.\n",
        "\n",
        "    Follows the *post-norm* convention used in the original GPT-1 paper:\n",
        "        x -> attention -> add & norm -> ffn -> add & norm\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim: int, num_heads: int, ff_dim: int,\n",
        "                 dropout: float = 0.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.attn = CausalSelfAttention(hidden_dim, num_heads, dropout)\n",
        "        self.ln1  = layers.LayerNormalization(epsilon=1e-5)\n",
        "        self.ffn  = keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation=\"gelu\"),\n",
        "            layers.Dense(hidden_dim),\n",
        "            layers.Dropout(dropout),\n",
        "        ], name=\"ffn\")\n",
        "        self.ln2 = layers.LayerNormalization(epsilon=1e-5)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        # Self-attention with residual connection and layer norm\n",
        "        x = self.ln1(x + self.attn(x, training=training))\n",
        "        # Feed-forward with residual connection and layer norm\n",
        "        x = self.ln2(x + self.ffn(x, training=training))\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 — Full GPT-1 Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class GPT1(keras.Model):\n",
        "    \"\"\"A minimal GPT-1 decoder-only transformer.\n",
        "\n",
        "    Components:\n",
        "      - Token embedding  (vocab_size → hidden_dim)\n",
        "      - Positional embedding (max_seq_len → hidden_dim)  — *learned*\n",
        "      - N × TransformerBlock\n",
        "      - Final LayerNorm\n",
        "      - Linear head projecting back to vocab_size (weight-tied with the\n",
        "        token embedding for parameter efficiency)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, max_seq_len: int, hidden_dim: int,\n",
        "                 num_layers: int, num_heads: int, ff_dim: int,\n",
        "                 dropout: float = 0.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.token_emb = layers.Embedding(vocab_size, hidden_dim, name=\"token_emb\")\n",
        "        self.pos_emb   = layers.Embedding(max_seq_len, hidden_dim, name=\"pos_emb\")\n",
        "        self.drop       = layers.Dropout(dropout)\n",
        "\n",
        "        self.blocks = [\n",
        "            TransformerBlock(hidden_dim, num_heads, ff_dim, dropout, name=f\"block_{i}\")\n",
        "            for i in range(num_layers)\n",
        "        ]\n",
        "\n",
        "        self.ln_f = layers.LayerNormalization(epsilon=1e-5, name=\"ln_f\")\n",
        "\n",
        "        # Output projection — we do NOT weight-tie here for clarity,\n",
        "        # but you could set  self.head.kernel = tf.transpose(self.token_emb.embeddings)\n",
        "        self.head = layers.Dense(vocab_size, use_bias=False, name=\"lm_head\")\n",
        "\n",
        "    def call(self, token_ids, training=False):\n",
        "        B = tf.shape(token_ids)[0]\n",
        "        T = tf.shape(token_ids)[1]\n",
        "\n",
        "        # Embeddings\n",
        "        positions = tf.range(T)[tf.newaxis, :]              # (1, T)\n",
        "        x = self.token_emb(token_ids) + self.pos_emb(positions)\n",
        "        x = self.drop(x, training=training)\n",
        "\n",
        "        # Transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x, training=training)\n",
        "\n",
        "        x = self.ln_f(x)                                    # (B, T, C)\n",
        "\n",
        "        # Project to vocabulary logits\n",
        "        logits = self.head(x)                                # (B, T, vocab_size)\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Instantiate the model\n",
        "model = GPT1(\n",
        "    vocab_size   = VOCAB_SIZE,\n",
        "    max_seq_len  = MAX_SEQ_LEN,\n",
        "    hidden_dim   = HIDDEN_DIM,\n",
        "    num_layers   = NUM_LAYERS,\n",
        "    num_heads    = NUM_HEADS,\n",
        "    ff_dim       = FF_DIM,\n",
        "    dropout      = DROPOUT_RATE,\n",
        ")\n",
        "\n",
        "# Build the model by running a dummy forward pass\n",
        "dummy_input = tf.zeros((1, MAX_SEQ_LEN), dtype=tf.int32)\n",
        "_ = model(dummy_input)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4 — Learning Rate Schedule\n",
        "\n",
        "GPT-1 uses a **linear warmup** followed by **cosine decay**, which is now\n",
        "standard for transformer training. We implement this as a custom Keras\n",
        "schedule."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class WarmupCosineSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \"\"\"Linear warmup followed by cosine decay.\"\"\"\n",
        "\n",
        "    def __init__(self, learning_rate: float, warmup_steps: int, total_steps: int):\n",
        "        super().__init__()\n",
        "        self.lr = learning_rate\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.total_steps = total_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)\n",
        "        warmup = tf.cast(self.warmup_steps, tf.float32)\n",
        "        total  = tf.cast(self.total_steps, tf.float32)\n",
        "\n",
        "        # Linear warmup\n",
        "        warmup_lr = self.lr * (step / tf.maximum(warmup, 1.0))\n",
        "\n",
        "        # Cosine decay\n",
        "        progress = (step - warmup) / tf.maximum(total - warmup, 1.0)\n",
        "        cosine_lr = self.lr * 0.5 * (1.0 + tf.cos(np.pi * progress))\n",
        "\n",
        "        return tf.where(step < warmup, warmup_lr, cosine_lr)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            \"learning_rate\": self.lr,\n",
        "            \"warmup_steps\": self.warmup_steps,\n",
        "            \"total_steps\": self.total_steps,\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualise the schedule\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "steps_per_epoch = len(train_data) // (MAX_SEQ_LEN * BATCH_SIZE)\n",
        "total_steps = steps_per_epoch * EPOCHS\n",
        "\n",
        "schedule = WarmupCosineSchedule(LEARNING_RATE, WARMUP_STEPS, total_steps)\n",
        "lr_values = [schedule(tf.constant(s, dtype=tf.float32)).numpy() for s in range(total_steps)]\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "plt.plot(lr_values)\n",
        "plt.xlabel(\"Training step\")\n",
        "plt.ylabel(\"Learning rate\")\n",
        "plt.title(\"Warmup + Cosine Decay Schedule\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Steps per epoch : {steps_per_epoch}\")\n",
        "print(f\"Total steps     : {total_steps}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5 — Compile & Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "optimizer = keras.optimizers.AdamW(\n",
        "    learning_rate=schedule,\n",
        "    weight_decay=0.01,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.98,\n",
        "    epsilon=1e-9,\n",
        "    clipnorm=1.0,\n",
        ")\n",
        "\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss_fn,\n",
        "    metrics=[\"accuracy\"],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\", patience=3, restore_best_weights=True\n",
        "    ),\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6 — Training Curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Loss\n",
        "axes[0].plot(history.history[\"loss\"], label=\"train\")\n",
        "axes[0].plot(history.history[\"val_loss\"], label=\"val\")\n",
        "axes[0].set_xlabel(\"Epoch\")\n",
        "axes[0].set_ylabel(\"Loss\")\n",
        "axes[0].set_title(\"Cross-Entropy Loss\")\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy\n",
        "axes[1].plot(history.history[\"accuracy\"], label=\"train\")\n",
        "axes[1].plot(history.history[\"val_accuracy\"], label=\"val\")\n",
        "axes[1].set_xlabel(\"Epoch\")\n",
        "axes[1].set_ylabel(\"Accuracy\")\n",
        "axes[1].set_title(\"Token-Level Accuracy\")\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "fig.suptitle(\"GPT-1 Training Curves\", fontsize=14, y=1.02)\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7 — Text Generation\n",
        "\n",
        "We implement **autoregressive generation** with temperature-controlled\n",
        "sampling and optional **top-k** filtering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def generate(\n",
        "    model: GPT1,\n",
        "    prompt: str,\n",
        "    max_new_tokens: int = 200,\n",
        "    temperature: float = 0.8,\n",
        "    top_k: int | None = 40,\n",
        ") -> str:\n",
        "    \"\"\"Generate text autoregressively from a prompt.\n",
        "\n",
        "    Args:\n",
        "        model: Trained GPT1 model.\n",
        "        prompt: Seed text.\n",
        "        max_new_tokens: Number of tokens to generate.\n",
        "        temperature: Softmax temperature (lower = more deterministic).\n",
        "        top_k: If set, only sample from the top-k most likely tokens.\n",
        "\n",
        "    Returns:\n",
        "        The full generated string (prompt + new tokens).\n",
        "    \"\"\"\n",
        "    token_ids = encode(prompt)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Crop to the last max_seq_len tokens (context window)\n",
        "        context = token_ids[-model.max_seq_len:]\n",
        "        x = tf.constant([context], dtype=tf.int32)\n",
        "\n",
        "        # Forward pass — only need the last position's logits\n",
        "        logits = model(x, training=False)          # (1, T, vocab_size)\n",
        "        logits = logits[0, -1, :] / temperature    # (vocab_size,)\n",
        "\n",
        "        # Optional top-k filtering\n",
        "        if top_k is not None:\n",
        "            top_values, _ = tf.math.top_k(logits, k=top_k)\n",
        "            threshold = top_values[-1]\n",
        "            logits = tf.where(logits < threshold, -1e9, logits)\n",
        "\n",
        "        # Sample from the distribution\n",
        "        probs = tf.nn.softmax(logits)\n",
        "        next_id = tf.random.categorical(\n",
        "            tf.math.log(probs)[tf.newaxis, :], num_samples=1\n",
        "        )\n",
        "        next_id = int(next_id[0, 0])\n",
        "\n",
        "        token_ids.append(next_id)\n",
        "\n",
        "    return decode(token_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generate samples with different temperatures\n",
        "prompts = [\"ROMEO:\", \"To be, or not to be\", \"The king\"]\n",
        "temperatures = [0.5, 0.8, 1.0]\n",
        "\n",
        "for prompt in prompts:\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Prompt: {prompt!r}\")\n",
        "    print(\"=\" * 70)\n",
        "    for temp in temperatures:\n",
        "        result = generate(model, prompt, max_new_tokens=150, temperature=temp)\n",
        "        print(f\"\\n--- temperature={temp} ---\")\n",
        "        print(result)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8 — Perplexity Evaluation\n",
        "\n",
        "**Perplexity** = exp(cross-entropy loss) is the standard metric for language\n",
        "models. Lower is better — it measures how \"surprised\" the model is by the\n",
        "validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "val_loss, val_acc = model.evaluate(val_ds, verbose=0)\n",
        "perplexity = np.exp(val_loss)\n",
        "\n",
        "print(f\"Validation loss      : {val_loss:.4f}\")\n",
        "print(f\"Validation accuracy  : {val_acc:.4f}\")\n",
        "print(f\"Validation perplexity: {perplexity:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 9 — Save & Load the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Save weights\n",
        "save_dir = os.path.join(\"..\", \"checkpoints\", \"gpt1_shakespeare\")\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "model.save_weights(os.path.join(save_dir, \"gpt1.weights.h5\"))\n",
        "print(f\"Weights saved to {save_dir}\")\n",
        "\n",
        "# To reload later:\n",
        "# model_loaded = GPT1(VOCAB_SIZE, MAX_SEQ_LEN, HIDDEN_DIM, NUM_LAYERS, NUM_HEADS, FF_DIM, DROPOUT_RATE)\n",
        "# model_loaded(dummy_input)  # build\n",
        "# model_loaded.load_weights(os.path.join(save_dir, \"gpt1.weights.h5\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Next Steps\n",
        "\n",
        "This notebook gives you a working GPT-1 baseline. Some ideas for extending it:\n",
        "\n",
        "- **BPE tokenisation** — swap the character tokeniser for a `tokenizers.ByteLevelBPETokenizer`\n",
        "  to handle a larger vocabulary and capture sub-word patterns.\n",
        "- **Pre-norm vs post-norm** — modern transformers (GPT-2+) use *pre-norm*\n",
        "  (LayerNorm before attention/FFN). Try switching and compare convergence.\n",
        "- **Weight tying** — tie the token embedding and the LM head weights to reduce\n",
        "  parameters and often improve performance.\n",
        "- **Larger data** — train on WikiText-103, OpenWebText, or The Pile.\n",
        "- **Scale up** — increase `NUM_LAYERS`, `HIDDEN_DIM`, `NUM_HEADS` toward the\n",
        "  original GPT-1 config (12, 768, 12) and compare.\n",
        "- **Fine-tuning** — add a classification head for downstream tasks (the\n",
        "  original GPT-1 innovation).\n",
        "- **Mixed precision** — use `tf.keras.mixed_precision` for faster training on\n",
        "  modern GPUs."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}