# Default experiment configuration
# Override via Hydra: python train.py model.hidden_dim=512

model:
  name: transformer
  num_layers: 6
  num_heads: 8
  hidden_dim: 256
  ff_dim: 1024
  dropout: 0.1
  max_seq_length: 512
  vocab_size: 30522  # BERT-base default

training:
  batch_size: 32
  epochs: 10
  learning_rate: 1.0e-4
  warmup_steps: 4000
  weight_decay: 0.01
  gradient_clip_norm: 1.0
  mixed_precision: true

data:
  dataset: null  # set per experiment
  train_split: "train"
  val_split: "validation"
  max_length: 512
  num_workers: 4

logging:
  experiment_name: "default"
  log_every_n_steps: 50
  save_checkpoints: true
  checkpoint_dir: "checkpoints"
  tensorboard: true
  wandb: false
